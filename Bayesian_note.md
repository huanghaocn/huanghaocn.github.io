##贝叶斯分类笔记

###信息论准则
此类准则将学习问题看作一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度。 
###最优化贝叶斯网络
这是一个NP难问题，有两种常用的策略能在有限时间内求得近似解：
第一种是贪婪法，从某个网络结构出发，每次调整一条边（增加、删除或调整方向），直到评分函数值不再降低为止；
第二种是通过给网络结构施加约束来削减搜索空间，例如将网络结构限定为树形结构等。
###吉布斯采样（Gibbs sampling）
这是一种随机采样的方法，它的工作原理如下：
令 $Q=\{Q_1,Q_2,...,Q_n\}$ 表示待查询变量，$E=\{E_1,E_2,...,E_k\}$ 为证据变量，已知其取值为 $e=\{e_1,e_2,...,e_k\}$ 目标是计算后验概率 $P(Q=q|E=e)$ ，其中 $q=\{q_1,q_2,...,q_n\}$ 是待查询变量的一组取值。  
###EM(Expectation-Maximization)算法
令 $\mathbf{X}$ 表示已观察变量集， $\mathbf{Z}$ 表示隐变量集，$\Theta $ 表示模型参数。若欲对 $\Theta $ 做极大似然估计，则应最大化对数似然
$$ LL(\Theta|\mathbf{X},\mathbf{Z})=\ln P(\mathbf{X},\mathbf{Z} | \Theta) $$
由于 $\mathbf{Z}$ 是隐变量，可通过对 $\mathbf{Z}$ 计算期望，来最大化已观察数据的对数“边际似然”
$$ LL(\Theta|\mathbf{X})=\ln P(\mathbf{X}|\Theta)=\ln \sum _\mathbf{Z} P(\mathbf{X},\mathbf{Z}|Theta) $$
EM算法是计算参数隐变量的重要方法。基本思想：若参数 $\Theta$ 已知，则可根据训练数据推断出最优隐变量 $\mathbf{Z}$ 的值（E步）；反之，若 $\mathbf{Z}$ 的值已知，则可方便地对参数 $\Theta$ 做极大似然估计。
以初始值 $\Theta^0$ 为起点，对上式可迭代执行以下步骤直至收敛：
+ 基于 $\Theta^t$ 推断隐变量 $\mathbf{Z}$ 的期望，记为  $\mathbf{Z}^t$ ;
+ 基于已观察变量 $\mathbf{X}$ 和 $\mathbf{Z}^t$ 对参数 $\Theta$ 做极大似然估计，记为 $\Theta^{t+1}$ ; 
